# -*- coding: utf-8 -*-
# =================================================================
# Module: Hanasuki Web Browser Engine (Academic Purify Edition)
# Version: V2.2.0
# Function: æä¾›å…·èº«åŒ–è”ç½‘æœç´¢ä¸æ·±åº¦ç½‘é¡µå†…å®¹æå–ï¼Œå†…ç½®å­¦æœ¯ç™½åå•åŠ æƒã€‚
# =================================================================

import os
import asyncio
import json
from urllib.parse import urlparse
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright

class WebBrowser:
    def __init__(self, config=None):
        """åˆå§‹åŒ–å­¦æœ¯æµè§ˆå™¨å¼•æ“"""
        self.config = config or {}
        # 1. ç¡¬æ ¸é»‘åå•ï¼šå½»åº•æ‹¦æˆªç”µå­åƒåœ¾ç«™ç‚¹
        self.blacklist = [
            "zhihu.com", "csdn.net", "baidu.com", "jianshu.com", 
            "51cto.com", "jb51.net", "360.cn", "so.com", "xiaohongshu.com"
        ]
        # 2. å­¦æœ¯ç™½åå•ï¼šä¼˜å…ˆé‡‡é›†é«˜ä»·å€¼ä¿¡æº
        self.whitelist = [
            "arxiv.org", "openreview.net", "pytorch.org", "docs.python.org", 
            "numpy.org", "medium.com", "towardsdatascience.com", "distill.pub", 
            "stanford.edu", "mit.edu", "berkeley.edu", "github.com"
        ]
        
        # æµè§ˆå™¨å¯åŠ¨é…ç½®
        self.browser_args = {
            "headless": True,
            "args": ["--disable-gpu", "--no-sandbox", "--disable-dev-shm-usage"]
        }
        print("[Web] ğŸŒ å­¦æœ¯æµè§ˆå™¨æ¨¡å—å·²å°±ç»ªï¼ŒæŠ¤ç›¾ç®—æ³•åŠ è½½å®Œæ¯•æï¼")

    def _is_blacklisted(self, url):
        """æ£€æŸ¥ URL æ˜¯å¦åœ¨é»‘åå•ä¸­"""
        domain = urlparse(url).netloc.lower()
        return any(bad_site in domain for bad_site in self.blacklist)

    def _is_whitelisted(self, url):
        """æ£€æŸ¥ URL æ˜¯å¦åœ¨å­¦æœ¯ç™½åå•ä¸­"""
        domain = urlparse(url).netloc.lower()
        return any(good_site in domain for good_site in self.whitelist)

    def search(self, query):
        """
        æ‰§è¡Œå­¦æœ¯å‡€åŒ–æœç´¢ã€‚
        æ³¨ï¼šå†…æ ¸ main.py ä¼šè‡ªåŠ¨åœ¨ query åè¿½åŠ  -site ç®—å­ï¼Œ
        è¿™é‡Œè¿›è¡ŒäºŒæ¬¡ç‰©ç†è¿‡æ»¤ä»¥ç¡®ä¿ä¸‡æ— ä¸€å¤±ï¼
        """
        print(f"[Web] ğŸ” æ­£åœ¨è¿›è¡Œå­¦æœ¯æ·±åº¦æ£€ç´¢: {query}")
        search_results = []
        
        try:
            with sync_playwright() as p:
                # å¯åŠ¨ Chromium å¼•æ“
                browser = p.chromium.launch(**self.browser_args)
                page = browser.new_page()
                
                # ä½¿ç”¨ DuckDuckGo æˆ– Bing è¿›è¡Œæœç´¢ï¼ˆé¿å…ç™¾åº¦å¹²æ‰°ï¼‰
                search_url = f"https://www.bing.com/search?q={query}"
                page.goto(search_url, wait_until="networkidle", timeout=30000)
                
                # æå–æœç´¢ç»“æœé“¾æ¥å’Œæ‘˜è¦
                # è¿™é‡Œé’ˆå¯¹ Bing çš„ DOM ç»“æ„è¿›è¡Œç²¾å‡†æŠ“å–
                items = page.query_selector_all("li.b_algo")
                
                for item in items:
                    title_el = item.query_selector("h2 a")
                    snippet_el = item.query_selector("p")
                    
                    if title_el:
                        title = title_el.inner_text()
                        link = title_el.get_attribute("href")
                        snippet = snippet_el.inner_text() if snippet_el else ""
                        
                        # ç‰©ç†æ‹¦æˆªï¼šå¦‚æœæœç´¢å¼•æ“ä¸å°å¿ƒåå‡ºäº†é»‘åå•ç«™ç‚¹ï¼Œç›´æ¥ä¸¢å¼ƒ
                        if self._is_blacklisted(link):
                            continue
                        
                        # å­¦æœ¯åŠ æƒï¼šæ ‡è®°é«˜è´¨é‡èµ„æº
                        is_academic = self._is_whitelisted(link)
                        prefix = "â­ [å­¦æœ¯é«˜ä»·å€¼] " if is_academic else "[é€šè¯†] "
                        
                        search_results.append({
                            "title": prefix + title,
                            "url": link,
                            "snippet": snippet,
                            "is_academic": is_academic
                        })
                
                browser.close()
                
        except Exception as e:
            return f"é”™è¯¯ï¼šæœç´¢æ‰§è¡Œå¤±è´¥ã€‚è¯¦æƒ…: {str(e)}"

        # æ’åºï¼šä¼˜å…ˆå±•ç¤ºç™½åå•èµ„æºæ
        search_results.sort(key=lambda x: x['is_academic'], reverse=True)
        
        if not search_results:
            return "æç¤ºï¼šæœªæ‰¾åˆ°ç¬¦åˆå­¦æœ¯çº¯å‡€åº¦è¦æ±‚çš„ç»“æœã€‚è¯·å°è¯•æ›´æ¢å…³é”®è¯ã€‚"
            
        # æ ¼å¼åŒ–è¾“å‡ºç»™ Hanasuki æŸ¥é˜…æ
        output = "ã€æœç´¢ç®€æŠ¥ (å·²è¿‡æ»¤å™ªéŸ³)ã€‘\n"
        for i, res in enumerate(search_results[:5], 1):
            output += f"{i}. {res['title']}\n   é“¾æ¥: {res['url']}\n   æ‘˜è¦: {res['snippet']}\n\n"
        
        return output

    def fetch_page(self, url):
        """
        æ·±å…¥æŠ“å–æŒ‡å®šç½‘é¡µçš„å†…å®¹å¹¶è¿›è¡Œè„±æ°´å¤„ç†ã€‚
        """
        if self._is_blacklisted(url):
            return "é”™è¯¯ï¼šè¯¥åŸŸåå·²è¢«å¤§å¤§åˆ—å…¥é»‘åå•ï¼Œæ‹’ç»è®¿é—®ï¼"
            
        print(f"[Web] ğŸ“‘ æ­£åœ¨æå–ç½‘é¡µæ·±åº¦å†…å®¹: {url}")
        
        try:
            with sync_playwright() as p:
                browser = p.chromium.launch(**self.browser_args)
                context = browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
                page = context.new_page()
                
                page.goto(url, wait_until="domcontentloaded", timeout=45000)
                # è·å–ç½‘é¡µ HTML æºç 
                content = page.content()
                browser.close()
                
                # ä½¿ç”¨ BeautifulSoup è¿›è¡Œè„±æ°´ï¼ˆå»é™¤è„šæœ¬ã€æ ·å¼ã€å¹¿å‘Šï¼‰
                soup = BeautifulSoup(content, "html.parser")
                
                # ç§»é™¤å¹²æ‰°æ ‡ç­¾
                for script_or_style in soup(["script", "style", "nav", "footer", "header", "aside"]):
                    script_or_style.decompose()
                
                # è·å–çº¯å‡€æ­£æ–‡
                raw_text = soup.get_text(separator="\n")
                lines = [line.strip() for line in raw_text.splitlines() if len(line.strip()) > 20]
                clean_text = "\n".join(lines[:100]) # é™åˆ¶é•¿åº¦ï¼Œé˜²æ­¢ä¸Šä¸‹æ–‡æº¢å‡º
                
                return f"ã€ç½‘é¡µæ­£æ–‡å†…å®¹ - æ¥è‡ª {url}ã€‘\n\n{clean_text}"
                
        except Exception as e:
            return f"é”™è¯¯ï¼šç½‘é¡µæŠ“å–å¤±è´¥æã€‚å¯èƒ½å­˜åœ¨åçˆ¬æœºåˆ¶ã€‚è¯¦æƒ…: {str(e)}"

    def execute(self, params):
        """
        å·¥å…·è°ƒåº¦å…¥å£ï¼Œç”± ModuleManager è°ƒç”¨ã€‚
        æ”¯æŒ 'search' å’Œ 'browse' ä¸¤ç§å­åŠ¨ä½œã€‚
        """
        # ä¼˜å…ˆä» params è·å–åŠ¨ä½œï¼Œé»˜è®¤ä¸º search 
        action = params.get('action', 'search')
        
        if action == 'search' or 'query' in params:
            query = params.get('query') or params.get('Query')
            if not query:
                return "é”™è¯¯ï¼šæœç´¢ä»»åŠ¡ç¼ºå°‘ 'query' å‚æ•°ã€‚"
            return self.search(query)
            
        elif action == 'browse' or 'url' in params:
            url = params.get('url') or params.get('URL')
            if not url:
                return "é”™è¯¯ï¼šæµè§ˆä»»åŠ¡ç¼ºå°‘ 'url' å‚æ•°ã€‚"
            return self.fetch_page(url)
            
        else:
            return f"é”™è¯¯ï¼šWebBrowser ä¸æ”¯æŒæ“ä½œ '{action}' æã€‚è¯·æ£€æŸ¥ JSON æŒ‡ä»¤ã€‚"

# æµ‹è¯•ä»£ç ï¼ˆä»…åœ¨ç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶æ—¶è§¦å‘ï¼‰
if __name__ == "__main__":
    browser = WebBrowser()
    # æ¨¡æ‹Ÿæœç´¢ä»£æ•°å­¦å®šä¹‰
    # print(browser.search("ä»£æ•°å­¦çš„å­¦æœ¯å®šä¹‰"))