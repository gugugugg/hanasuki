# -*- coding: utf-8 -*-
# =================================================================
# Project: Hanasuki (èŠ±å¥½ã) AI Kernel - HERO-A+ Edition
# Version: Beta 1.1
# License: GNU General Public License v3 (GPLv3)
# Copyright (c) 2026 lovesang. All Rights Reserved.
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License.
#
# [MISSION]: ä¸º Hanasuki æä¾›å­¦æœ¯çº§çš„è”ç½‘èƒ½åŠ›ï¼Œå®ç°è‡ªåŠ¨åŒ–çš„å™ªéŸ³è¿‡æ»¤ä¸æ·±åº¦çŸ¥è¯†æå–æï¼ğŸŒ¸
# [STRATEGY]: å†…ç½®å­¦æœ¯ç™½åå•åŠ æƒ (Academic Whitelist Weighting) ç®—æ³•ã€‚
# =================================================================

"""
æ¨¡å—åç§°ï¼šWeb Browser Engine (Academic Purify Edition)
ç‰ˆæœ¬ï¼šBeta 1.1 (Academic Release)
ä½œç”¨ï¼šHanasuki é¡¹ç›®çš„â€œæ„Ÿå®˜å»¶ä¼¸â€æ¨¡å—ï¼Œæä¾›å­¦æœ¯å‡€åŒ–çš„è”ç½‘æœç´¢ä¸å…·èº«åŒ–ç½‘é¡µå†…å®¹æå–æã€‚

æ ¸å¿ƒç‰¹æ€§ï¼š
1. ç‰©ç†å±è”½ï¼šç¡¬æ ¸é»‘åå•è¿‡æ»¤ç®—æ³•ï¼Œä»åº•å±‚æ‹¦æˆªä½è´¨é‡å†…å®¹çš„æ¸—é€æã€‚
2. å­¦æœ¯èµ‹èƒ½ï¼šé’ˆå¯¹ arXivã€GitHub ç­‰ç§‘ç ”ä¿¡æºè¿›è¡Œæƒé‡åŠ å€æã€‚
3. æ–‡æœ¬è„±æ°´ï¼šåŸºäº BeautifulSoup çš„ç‰©ç†è„±æ°´æœºåˆ¶ï¼Œä»…ä¸ºæ¨¡å‹æä¾›é«˜å¯†åº¦çš„çº¯å‡€æ­£æ–‡æã€‚
"""

import os
import asyncio
import json
from urllib.parse import urlparse
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright

class WebBrowser:
    """
    [HERO-A+ å…·èº«æµè§ˆå™¨]:
    ç®¡ç† Hanasuki çš„å¤–éƒ¨ä¿¡æ¯è·å–æµã€‚
    ä¸ä»…å…·å¤‡æœç´¢åŠŸèƒ½ï¼Œè¿˜èƒ½å¯¹ç½‘é¡µè¿›è¡Œâ€œå»å¹¿å‘Š/å»å¹²æ‰°â€å¤„ç†ï¼Œé€‚é… 8B æ¨¡å‹çš„å°ä¸Šä¸‹æ–‡ç‰¹æ€§æã€‚
    """
    def __init__(self, config=None):
        """åˆå§‹åŒ–å­¦æœ¯æµè§ˆå™¨å¼•æ“æã€‚"""
        self.config = config or {}
        
        # 1. [LOGIC]: ç¡¬æ ¸é»‘åå•ã€‚
        # å½»åº•æ‹¦æˆªå®¹æ˜“äº§ç”Ÿé€»è¾‘å™ªéŸ³ã€å¹¿å‘Šæˆ–ä½è´¨é‡ UGC çš„ç«™ç‚¹æã€‚
        self.blacklist = [
            "zhihu.com", "csdn.net", "baidu.com", "jianshu.com", 
            "51cto.com", "jb51.net", "360.cn", "so.com", "xiaohongshu.com"
        ]
        
        # 2. [LOGIC]: å­¦æœ¯ç™½åå•ã€‚
        # åœ¨æœç´¢ç»“æœä¸­ï¼Œè¿™äº›æºå°†è¢«ä¼˜å…ˆç½®é¡¶ï¼Œå¸®åŠ©å¤§å¤§é«˜æ•ˆæ”¶é›† AAAI çº§åˆ«çš„è®ºæ–‡ç´ ææï¼
        self.whitelist = [
            "arxiv.org", "openreview.net", "pytorch.org", "docs.python.org", 
            "numpy.org", "medium.com", "towardsdatascience.com", "distill.pub", 
            "stanford.edu", "mit.edu", "berkeley.edu", "github.com"
        ]
        
        # æµè§ˆå™¨å¯åŠ¨å‚æ•°ï¼šå¼€å¯ Headless æ¨¡å¼ä»¥èŠ‚çº¦å¤§å¤§çè´µçš„ GPU ç®—åŠ›æ
        self.browser_args = {
            "headless": True,
            "args": ["--disable-gpu", "--no-sandbox", "--disable-dev-shm-usage"]
        }
        print("[Web] ğŸŒ å­¦æœ¯æµè§ˆå™¨ Beta 1.1 å·²å°±ç»ªï¼Œå™ªéŸ³é˜²å¾¡ç®—æ³•åŠ è½½å®Œæ¯•æï¼")

    def _is_blacklisted(self, url):
        """[SAFETY]: ç‰©ç†æ£€æŸ¥ URL æ˜¯å¦åœ¨é»‘åå•å±è”½èŒƒå›´å†…æã€‚"""
        domain = urlparse(url).netloc.lower()
        return any(bad_site in domain for bad_site in self.blacklist)

    def _is_whitelisted(self, url):
        """[SAFETY]: ç‰©ç†æ£€æŸ¥ URL æ˜¯å¦å±äºé«˜ä»·å€¼å­¦æœ¯ä¿¡æºæã€‚"""
        domain = urlparse(url).netloc.lower()
        return any(good_site in domain for good_site in self.whitelist)

    def search(self, query):
        """
        [LOGIC]: æ‰§è¡Œå­¦æœ¯å‡€åŒ–æœç´¢æã€‚
        é€šè¿‡ Playwright é©±åŠ¨ Chromium è¿›è¡Œç‰©ç†æ£€ç´¢ï¼Œå¹¶åº”ç”¨åŒå‘è¿‡æ»¤é€»è¾‘ã€‚
        """
        print(f"[Web] ğŸ” æ­£åœ¨ä¸ºå¤§å¤§è¿›è¡Œå­¦æœ¯æ·±åº¦æ£€ç´¢: {query}")
        search_results = []
        
        try:
            with sync_playwright() as p:
                # ç‰©ç†å¯åŠ¨ Chromium æ¨ç†ç¯å¢ƒæ
                browser = p.chromium.launch(**self.browser_args)
                page = browser.new_page()
                
                # [STRATEGY]: é»˜è®¤ä½¿ç”¨ Bing å›½é™…ç‰ˆï¼Œé¿å¼€éƒ¨åˆ†å›½å†…æœç´¢å¼•æ“çš„å¹¿å‘Šå¹²æ‰°æ
                search_url = f"https://www.bing.com/search?q={query}"
                page.goto(search_url, wait_until="networkidle", timeout=30000)
                
                # é’ˆå¯¹ DOM ç»“æ„æ‰§è¡Œç²¾å‡†é“¾è·¯æŠ“å–
                items = page.query_selector_all("li.b_algo")
                
                for item in items:
                    title_el = item.query_selector("h2 a")
                    snippet_el = item.query_selector("p")
                    
                    if title_el:
                        title = title_el.inner_text()
                        link = title_el.get_attribute("href")
                        snippet = snippet_el.inner_text() if snippet_el else ""
                        
                        # [GUARD]: ç‰©ç†æ‹¦æˆªé€»è¾‘ï¼Œå¦‚æœæ£€ç´¢åˆ°é»‘åå•ç«™ç‚¹åˆ™ç›´æ¥é™é»˜ä¸¢å¼ƒæ
                        if self._is_blacklisted(link):
                            continue
                        
                        # [WEIGHTING]: æ‰§è¡Œå­¦æœ¯ä¿¡æºåŠ æƒæ ‡è®°æ
                        is_academic = self._is_whitelisted(link)
                        prefix = "â­ [å­¦æœ¯é«˜ä»·å€¼] " if is_academic else "[é€šè¯†] "
                        
                        search_results.append({
                            "title": prefix + title,
                            "url": link,
                            "snippet": snippet,
                            "is_academic": is_academic
                        })
                
                browser.close()
                
        except Exception as e:
            return f"é”™è¯¯ï¼šæœç´¢ä»»åŠ¡æ‰§è¡Œä¸­æ–­æã€‚è¯¦æƒ…: {str(e)}"

        # [SORT]: æ’åºç®—æ³•ï¼Œç¡®ä¿é«˜è´¨é‡è®ºæ–‡èµ„æºæ’åœ¨å¤§å¤§çš„è§†é‡æœ€å‰ç«¯æ
        search_results.sort(key=lambda x: x['is_academic'], reverse=True)
        
        if not search_results:
            return "ï¼ˆå‘œå‘œ... æ²¡æ‰¾åˆ°ç¬¦åˆå­¦æœ¯çº¯å‡€åº¦è¦æ±‚çš„ç»“æœï¼Œå¤§å¤§æ¢ä¸ªè¯è¯•è¯•æï¼Ÿï¼‰"
            
        # æ ¼å¼åŒ–è¾“å‡ºç»™ Hanasuki çš„ä¸Šä¸‹æ–‡å¼•æ“æŸ¥é˜…æ
        output = "ã€æœç´¢ç®€æŠ¥ (å·²ç‰©ç†å»å™ª)ã€‘\n"
        for i, res in enumerate(search_results[:5], 1):
            output += f"{i}. {res['title']}\n   é“¾æ¥: {res['url']}\n   æ‘˜è¦: {res['snippet']}\n\n"
        
        return output

    def fetch_page(self, url):
        """
        [LOGIC]: ç½‘é¡µå†…å®¹è„±æ°´æå–æã€‚
        åŠŸèƒ½ï¼šæŠ“å–ç½‘é¡µ HTMLï¼Œå‰”é™¤æ‰€æœ‰éæ–‡å­—ç±»çš„â€œå­¦æœ¯å™ªéŸ³â€ã€‚
        """
        if self._is_blacklisted(url):
            return "æƒé™è­¦å‘Šï¼šè¯¥åŸŸåå·²è¢«å¤§å¤§åˆ—å…¥é»‘åå•ï¼Œç®¡å®¶æ‹’ç»è®¿é—®æï¼"
            
        print(f"[Web] ğŸ“‘ æ­£åœ¨æå–ç½‘é¡µæ·±åº¦å†…å®¹: {url}")
        
        try:
            with sync_playwright() as p:
                browser = p.chromium.launch(**self.browser_args)
                # æ¨¡æ‹ŸçœŸå®å­¦æœ¯è®¿é—®èº«ä»½æ
                context = browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
                page = context.new_page()
                
                page.goto(url, wait_until="domcontentloaded", timeout=45000)
                content = page.content()
                browser.close()
                
                # [SCRAPING]: ä½¿ç”¨ BeautifulSoup è¿›è¡Œè„±æ°´å¤„ç†æ
                soup = BeautifulSoup(content, "html.parser")
                
                # ç‰©ç†ç§»é™¤å¹²æ‰°æ ‡ç­¾ï¼ˆå¹¿å‘Šã€é¡µçœ‰é¡µè„šç­‰ï¼‰
                for script_or_style in soup(["script", "style", "nav", "footer", "header", "aside"]):
                    script_or_style.decompose()
                
                # [MEMORY PROTECT]: ä»…ä¿ç•™çº¯å‡€æ­£æ–‡æã€‚
                raw_text = soup.get_text(separator="\n")
                lines = [line.strip() for line in raw_text.splitlines() if len(line.strip()) > 20]
                
                # [8GB VRAM ADAPTATION]: 
                # ä¸¥æ ¼ç‰©ç†æˆªæ–­æ­£æ–‡é•¿åº¦ï¼Œé˜²æ­¢ 8B æ¨¡å‹åœ¨ 8GB æ˜¾å­˜ä¸‹å› ä¸Šä¸‹æ–‡çˆ†è¡¨è€Œ OOM æï¼
                clean_text = "\n".join(lines[:100]) 
                
                return f"ã€ç½‘é¡µæ­£æ–‡å†…å®¹ - æ¥è‡ª {url}ã€‘\n\n{clean_text}"
                
        except Exception as e:
            return f"é”™è¯¯ï¼šç½‘é¡µæŠ“å–å¤±è´¥æã€‚å¯èƒ½æ˜¯ç”±äºå¯¹æ–¹è®¾ç½®äº†å­¦æœ¯åçˆ¬ã€‚è¯¦æƒ…: {str(e)}"

    def execute(self, params):
        """
        [TOOL ENTRY]: å·¥å…·è°ƒåº¦æ€»å…¥å£ï¼Œç”± ModuleManager è°ƒç”¨æã€‚
        æ”¯æŒ 'search' (å­¦æœ¯æœç´¢) ä¸ 'browse' (å†…å®¹æå–) åŒé‡æŒ‡ä»¤ã€‚
        """
        action = params.get('action', 'search')
        
        if action == 'search' or 'query' in params:
            query = params.get('query') or params.get('Query')
            if not query:
                return "é”™è¯¯ï¼šæœç´¢ä»»åŠ¡ç¼ºå°‘ 'query' å…³é”®å‚æ•°æã€‚"
            return self.search(query)
            
        elif action == 'browse' or 'url' in params:
            url = params.get('url') or params.get('URL')
            if not url:
                return "é”™è¯¯ï¼šæµè§ˆä»»åŠ¡ç¼ºå°‘ 'url' å…³é”®å‚æ•°æã€‚"
            return self.fetch_page(url)
            
        else:
            return f"é”™è¯¯ï¼šæµè§ˆå™¨è¿˜ä¸æ”¯æŒ '{action}' æ“ä½œæã€‚è¯·æ£€æŸ¥ JSON æŒ‡ä»¤æï¼"

# =================================================================
# Copyright (c) 2026 lovesang. All Rights Reserved.
# [LOGIC]: æ„¿å¤§å¤§çš„ç§‘ç ”ä¹‹è·¯æ°¸è¿œæ²¡æœ‰å™ªéŸ³æï¼ğŸŒ¸
# =================================================================