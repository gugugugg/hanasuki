# -*- coding: utf-8 -*-
# =================================================================
# Project: Hanasuki (èŠ±å¥½ã) AI Kernel - HERO-A+ Edition
# Version: Beta 1.1
# License: GNU General Public License v3 (GPLv3)
# Copyright (c) 2026 lovesang. All Rights Reserved.
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License.
#
# [MISSION]: åœ¨ RTX 5060 (8GB) çš„ä¸¥è‹›ç¯å¢ƒä¸‹ï¼Œé€šè¿‡ KV å‹ç¼©å®ç°è¶…é•¿ä¸Šä¸‹æ–‡æ¨ç†æï¼ğŸŒ¸
# [TECH]: åŸºäº llama-cpp-python å®ç°çš„é‡åŒ–åŠ é€Ÿåç«¯ã€‚
# =================================================================

"""
æ¨¡å—åç§°ï¼šLlamaBackend (HERO-A+ æ˜¾å­˜ä¼˜åŒ–ç‰ˆ)
ç‰ˆæœ¬ï¼šBeta 1.1 (Academic Speculative Ready)
ä½œç”¨ï¼šHanasuki é¡¹ç›®çš„æ¨ç†ä¸­æ¢ã€‚
æ ¸å¿ƒä¼˜åŒ–ï¼š
1. [VRAM Saver]: é’ˆå¯¹ 8GB æ˜¾å­˜å¼•å…¥ KV Cache 4-bit é‡åŒ– (Q4_K)ï¼Œç‰©ç†èŠ‚çº¦ 50% çš„ç¼“å­˜å¼€é”€ã€‚
2. [Dynamic Reload]: æ”¯æŒâ€œæ—¥å¸¸â€ä¸â€œè‡ªç ”â€æ¨¡å¼é—´çš„ä¸Šä¸‹æ–‡é•¿åº¦ (n_ctx) æ— ç¼åˆ‡æ¢æã€‚
3. [Safety Guard]: å†…ç½®æ˜¾å­˜æº¢å‡º (OOM) ç´§æ€¥å›æ»šé€»è¾‘ï¼Œç¡®ä¿å†…æ ¸ä¸é—ªé€€æã€‚
"""

import os
import gc
import sys

try:
    # [LOGIC]: é‡‡ç”¨å»¶è¿Ÿå¯¼å…¥ç­–ç•¥ã€‚
    # åªæœ‰å½“ç”¨æˆ·åœ¨ config.yaml ä¸­æŒ‡å®šä½¿ç”¨æœ¬åç«¯æ—¶ï¼Œæ‰åŠ è½½åº•å±‚æƒé‡å¤„ç†åº“æã€‚
    from llama_cpp import Llama, ggml
except ImportError:
    raise ImportError("ç¼ºå°‘æ ¸å¿ƒä¾èµ–åº“æï¼šè¯·è¿è¡Œ pip install llama-cpp-python ä»¥æ¿€æ´»å¤§è„‘æï¼")

class LlamaBackend:
    """
    [HERO-A+ æ¨ç†æ ¸å¿ƒ]:
    å°è£…äº†åº•å±‚ GGUF æ¨¡å‹åŠ è½½ä¸ Chat Completion æ¥å£æã€‚
    """
    def __init__(self, config):
        """
        [LOGIC]: åˆå§‹åŒ– Llama æ¨ç†å¼•æ“æã€‚
        æ ¹æ®é…ç½®è‡ªåŠ¨è¯†åˆ« Profile å¹¶å»ºç«‹ç‰©ç†è¿æ¥æã€‚
        """
        self.config = config
        self.base_path = config.get('path')
        self.model = None
        self._resolve_path()
        
        # åˆå§‹åŠ è½½æ—¶ä¼˜å…ˆä½¿ç”¨æ—¥å¸¸äº¤äº’æ¨¡å¼ (profile_normal) æ
        init_profile = config.get('profile_normal', config)
        self._load_model(init_profile)

    def _resolve_path(self):
        """[LOGIC]: ç‰©ç†è·¯å¾„è‡ªåŠ¨å¯¹é½é€»è¾‘æ"""
        if not os.path.isabs(self.base_path):
            # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼Œç¡®ä¿ Windows ç›¸å¯¹è·¯å¾„çš„é²æ£’æ€§æã€‚
            current_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
            self.base_path = os.path.join(os.path.dirname(current_dir), self.base_path)
        
        if not os.path.exists(self.base_path):
            print(f"[ç³»ç»Ÿ] âŒ æ‰¾ä¸åˆ°æ¨¡å‹æƒé‡æï¼š{self.base_path}")

    def _load_model(self, profile):
        """
        [HERO-A+ CORE]: æ˜¾å­˜èŠ‚æµåŠ è½½å¼•æ“æã€‚
        è¿™æ˜¯å®ç° AAAI çº§è®ºæ–‡å®éªŒæ•°æ®çš„æ ¸å¿ƒé€»è¾‘ã€‚
        é€šè¿‡é‡åŒ– Key/Value Cacheï¼Œåœ¨ 8GB æ˜¾å¡ä¸Šå®ç°æœ€é«˜ 16k çš„ç¨³å®šä¸Šä¸‹æ–‡æï¼
        """
        try:
            # 1. ç‰©ç†æ¸…ç†ï¼šåœ¨é‡è½½æ¨¡å‹å‰å½»åº•å›æ”¶æ—§æ˜¾å­˜ï¼Œé˜²æ­¢ç¢ç‰‡åŒ–å¼•å‘ OOM æã€‚
            if self.model:
                del self.model
                gc.collect()
                # è”åŠ¨æ¸…é™¤ PyTorch ç¼“å­˜ï¼ˆå¦‚æœç¯å¢ƒä¸­æœ‰å®‰è£…ï¼‰
                try:
                    import torch
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                except:
                    pass

            n_ctx = profile.get('n_ctx', 8192) # é»˜è®¤ä½¿ç”¨å¤§å¤§çš„ 8k è®¾ç½®
            print(f"[ç³»ç»Ÿ] ğŸš€ æ­£åœ¨è£…è½½ Qwen æ ¸å¿ƒé€»è¾‘ç©ºé—´ï¼Œn_ctx={n_ctx}...")

            # 2. è°ƒç”¨åº•å±‚ Llama æ„é€ å‡½æ•°ï¼Œæ³¨å…¥é‡åŒ–é»‘ç§‘æŠ€æã€‚
            self.model = Llama(
                model_path=self.base_path,
                n_gpu_layers=profile.get('n_gpu_layers', -1), # -1 ä»£è¡¨å…¨å±‚å¸è½½è‡³ GPU æ
                n_ctx=n_ctx,
                flash_attn=profile.get('flash_attn', True), # å¼€å¯ Flash Attention é™ä½è®¡ç®—å¤æ‚åº¦
                
                # [8GB VRAM OPTIMIZATION]: 
                # æ ¸å¿ƒé‡åŒ–å‚æ•°ã€‚å°† KV ç¼“å­˜å‹ç¼©ä¸º Q4_K æ ¼å¼ã€‚
                # è¿™èƒ½è®©åŸæœ¬å ç”¨ 2GB çš„ç¼“å­˜ç©ºé—´ç¼©å°åˆ°çº¦ 1GB æï¼
                type_k=ggml.GGML_TYPE_Q4_K, 
                type_v=ggml.GGML_TYPE_Q4_K,
                
                offload_kqv=True, # å¿…é¡»é”å®šåœ¨ GPU ä»¥ç»´æŒæ¨ç†å¸§ç‡æ
                verbose=False,
                seed=-1
            )
            print(f"[ç³»ç»Ÿ] âœ… Hanasuki çš„è®¤çŸ¥ç©ºé—´å·²å»ºç«‹ã€‚æ˜¾å­˜æŠ¤ç›¾ï¼š[å·²æ¿€æ´»] æï¼")

        except Exception as e:
            print(f"[ç³»ç»Ÿ] âš ï¸ è„‘ç»†èƒæ‰©å®¹å¤±è´¥æ: {e}")
            raise e

    def reload(self, profile):
        """
        [LOGIC]: æ¨¡å¼çƒ­åˆ‡æ¢ã€‚
        å½“å¤§å¤§å¼€å¯â€œè‡ªç ”æ¨¡å¼â€æ—¶ï¼Œè‡ªåŠ¨åŠ¨æ€è°ƒæ•´æ¨ç†åç«¯çš„ä¸Šä¸‹æ–‡æ·±åº¦æã€‚
        """
        print(f"[ç³»ç»Ÿ] ğŸ”„ æ­£åœ¨æ ¹æ®å­¦æœ¯éœ€æ±‚é‡ç»„æ¨ç†ç©ºé—´æ...")
        try:
            self._load_model(profile)
            print("[ç³»ç»Ÿ] åˆ‡æ¢å®Œæ¯•ï¼Œé•¿æ–‡æœ¬ç ”ç©¶èƒ½åŠ›å·²å°±ç»ªæï¼")
        except Exception as e:
            print(f"[ç³»ç»Ÿ] âš ï¸ æ˜¾å­˜ä¸è¶³ä»¥æ”¯æ’‘é•¿æ–‡æœ¬æ: {e}")
            # [FALLBACK]: ç´§æ€¥å›æ»šï¼Œé˜²æ­¢ç¨‹åºå½»åº•å´©æºƒ
            fallback_profile = {
                'n_gpu_layers': -1,
                'n_ctx': 4096,
                'flash_attn': True
            }
            try:
                self._load_model(fallback_profile)
                print("[ç³»ç»Ÿ] âœ… å·²å›é€€è‡³ 4k å®‰å…¨è¾¹ç•Œã€‚è™½ç„¶è®°æ€§å˜å·®äº†ï¼Œä½† Hanasuki è¿˜æ²¡æ™•å€’æï¼")
            except Exception as fatal_e:
                print(f"[ç³»ç»Ÿ] âŒ è‡´å‘½é”™è¯¯ï¼šå›æ»šå¤±è´¥ï¼Œè¯·æ‰‹åŠ¨é‡Šæ”¾æ˜¾å­˜è½¯ä»¶æï¼{fatal_e}")

    def generate(self, history, stream=True, stop=None):
        """
        [LOGIC]: æ ¸å¿ƒæ¨ç†ç”Ÿæˆæ¥å£ã€‚
        æ”¯æŒæµå¼åå­—ï¼Œè®©å¤§å¤§èƒ½å®æ—¶æ„Ÿå—åˆ° Hanasuki çš„æ€è€ƒè¿‡ç¨‹æã€‚
        """
        if self.model is None:
            msg = "ï¼ˆå†…æ ¸å¼‚å¸¸ï¼šHanasuki æ‰¾ä¸åˆ°å¤§è„‘æ¨¡å—æï¼Œè¯·æ£€æŸ¥æ¨¡å‹è·¯å¾„...ï¼‰"
            if stream: yield msg
            else: return msg
            return

        # é’ˆå¯¹ Qwen ç³»åˆ—ä¼˜åŒ–çš„åœç”¨è¯åˆ—è¡¨æ
        stop_words = stop if stop else ["<|im_end|>", "<|endoftext|>", "###"]

        try:
            # ç‰©ç†è°ƒç”¨æ¨ç†ç”Ÿæˆ
            response = self.model.create_chat_completion(
                messages=history,
                stream=stream,
                stop=stop_words,
                temperature=self.config.get('temperature', 0.7),
                repeat_penalty=1.1 # ç‰©ç†æ‹¦æˆªå¤è¯»æœºå€¾å‘æ
            )

            if stream:
                for chunk in response:
                    delta = chunk['choices'][0]['delta']
                    if 'content' in delta:
                        yield delta['content']
            else:
                return response['choices'][0]['message']['content']

        except Exception as e:
            error_msg = f"ï¼ˆå‘œå‘œ... åˆšæ‰æ€ç»´çŸ­è·¯äº†æ... è¯¦æƒ…: {e}ï¼‰"
            if stream: yield error_msg
            else: return error_msg