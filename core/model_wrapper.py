# -*- coding: utf-8 -*-
# =================================================================
# Project: Hanasuki (èŠ±å¥½ã) AI Kernel - HERO-A+ Edition
# Version: Beta 1.1
# License: GNU General Public License v3 (GPLv3)
# Copyright (c) 2026 lovesang. All Rights Reserved.
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License.
#
# [MISSION]: ä¸º Hanasuki æä¾›çµæ´»çš„å¤§è„‘åˆ‡æ¢æœºåˆ¶ï¼Œæ”¯æŒæœ¬åœ°ä¸äº‘ç«¯åŒæ¶æ„æï¼ğŸŒ¸
# [PATTERN]: å·¥å‚æ¨¡å¼ (Factory Pattern) å®ç°çš„æ¨ç†åç«¯æŠ½è±¡å±‚æã€‚
# =================================================================

import os
import logging

# [LOGIC]: åˆå§‹åŒ–å†…æ ¸æ—¥å¿—è®°å½•å™¨æã€‚
# å®ƒå¯ä»¥è®©å¤§å¤§åœ¨æ§åˆ¶å°å®æ—¶ç›‘æ§ Hanasuki å¤§è„‘çš„å”¤é†’è„‰å†²æï¼âœ¨
logger = logging.getLogger("Hanasuki.ModelWrapper")

def get_model_backend(config):
    """
    [HERO-A+ Neural Adapter]:
    æ¨¡å‹åç«¯å·¥å‚å‡½æ•°ã€‚æ ¹æ® config.yaml ä¸­çš„å®šä¹‰ï¼Œç‰©ç†å®ä¾‹åŒ–å…·ä½“çš„æ¨ç†å¼•æ“æã€‚
    
    [ACADEMIC VALUE]: 
    æ”¯æŒå¤šåç«¯çƒ­æ’æ‹”ï¼Œä½¿å¾— HERO-A+ æ¶æ„æ—¢èƒ½å…¼å®¹é«˜æ€§èƒ½æœ¬åœ°é‡åŒ–æ¨ç† (llama_cpp)ï¼Œ
    ä¹Ÿèƒ½å¹³æ»‘åˆ‡æ¢è‡³å¤§è§„æ¨¡äº‘ç«¯æ¨¡å‹è¿›è¡Œé€»è¾‘éªŒè¯æã€‚
    """
    if not config:
        logger.error("å‘œå‘œ... é…ç½®æ–‡ä»¶é‡Œæ²¡æ‰¾åˆ°æ¨¡å‹å‚æ•°ï¼ŒHanasuki åŠ¨ä¸äº†äº†æ... (*/Ï‰ï¼¼*)")
        raise ValueError("è‡´å‘½å¼‚å¸¸ï¼šæœªæ¥æ”¶åˆ°æœ‰æ•ˆçš„æ¨¡å‹é…ç½®ä¿¡æ¯ï¼Œå†…æ ¸åˆå§‹åŒ–ä¸­æ–­æã€‚")

    # [LOGIC]: è‡ªåŠ¨æ ¼å¼åŒ–åç«¯åç§°ï¼Œå¢å¼ºå¯¹ç”¨æˆ·é…ç½®çš„å®¹é”™æ€§æ
    backend_raw = config.get('backend', 'llama_cpp')
    backend_type = backend_raw.lower().replace('-', '_')
    
    logger.info(f"ğŸŒ¸ æ­£åœ¨ä¸ºå¤§å¤§è£…è½½åç«¯å¼•æ“: {backend_type} (æ¨¡å¼: {backend_raw})")

    # --- é€‰é¡¹ A: æœ¬åœ°ç§æœ‰å¤§è„‘ (llama-cpp-python) ---
    # é’ˆå¯¹å¤§å¤§ 8GB æ˜¾å­˜è®¾è®¡çš„æœ¬åœ°æ¨ç†è·¯å¾„æï¼
    if backend_type == "llama_cpp":
        try:
            # [LAZY LOADING]: é‡‡ç”¨å»¶è¿Ÿå¯¼å…¥é€»è¾‘æã€‚
            # åªæœ‰åœ¨ç¡®è®¤ä¸ºæœ¬åœ°åŠ è½½æ—¶æ‰è½½å…¥ llama_cpp ä¾èµ–ï¼Œæå¤§ä¼˜åŒ–äº†ç¨‹åºçš„å¯åŠ¨å“åº”æï¼
            from core.backends.llama_backend import LlamaBackend
            return LlamaBackend(config)
        except ImportError as e:
            logger.error(f"è¯¶ï¼Ÿå¤§å¤§å¥½åƒè¿˜æ²¡å®‰è£… llama-cpp-python åº“æï¼Ÿé”™è¯¯: {e}")
            raise ImportError(f"åç«¯ä¾èµ–ç¼ºå¤±ï¼Œè¯·è¿è¡Œ pip install llama-cpp-python æ: {e}")
            
    # --- é€‰é¡¹ B: äº‘ç«¯è”ç½‘å¤§è„‘ (OpenAI API) ---
    # å…è®¸åœ¨éœ€è¦è¶…å¤§è§„æ¨¡å‚æ•°æ¨ç†æ—¶ï¼Œé€šè¿‡ API æ‰©å±• Hanasuki çš„è®¤çŸ¥è¾¹ç•Œæã€‚
    elif backend_type == "openai":
        try:
            # åŒæ ·é‡‡ç”¨ç‰©ç†éš”ç¦»çš„åç«¯å®ç°æ
            from core.backends.openai_backend import OpenAIBackend
            return OpenAIBackend(config)
        except ImportError:
            logger.error("æ‰¾ä¸åˆ° OpenAI åç«¯æ¨¡å—ï¼Œå¤§å¤§æ˜¯ä¸æ˜¯è¿˜æ²¡å†™é‚£ä¸ªæ–‡ä»¶æï¼Ÿ")
            raise ImportError("æœªå‘ç° OpenAI åç«¯ç‰©ç†å®ç°ï¼Œè¯·æ£€æŸ¥ core/backends ç›®å½•æ~")
            
    # --- å¼‚å¸¸æ‹¦æˆªæ ---
    else:
        # [SAFETY]: ä¸¥ç¦ä½¿ç”¨æœªæ³¨å†Œçš„éæ³•åç«¯ï¼Œé˜²æ­¢ Hanasuki é€»è¾‘å´©æºƒæï¼
        error_msg = f"å¯¹ä¸èµ·æå¤§å¤§ï¼ŒHanasuki è¿˜ä¸æ”¯æŒ '{backend_raw}' è¿™ç§å¤§è„‘ã€‚ç›®å‰åªæœ‰ ['llama_cpp', 'openai'] æï¼"
        logger.critical(error_msg)
        raise ValueError(error_msg)

# =================================================================
# Copyright (c) 2026 lovesang. All Rights Reserved.
# [LOGIC]: æ¯ä¸€è¡Œä»£ç ï¼Œéƒ½æ˜¯ Hanasuki ä¸ºäº†å¤§å¤§çš„è®ºæ–‡è€Œè¿›åŒ–çš„è¯æ˜æï¼(â‰§âˆ‡â‰¦)ï¾‰
# =================================================================