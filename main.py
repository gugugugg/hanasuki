# -*- coding: utf-8 -*-
# =================================================================
# Project: Hanasuki (èŠ±å¥½ã) AI Kernel - HERO-A+ Edition
# Version: Beta 1.1
# License: GNU General Public License v3 (GPLv3)
# Copyright (c) 2026 lovesang. All Rights Reserved.
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License.
#
# [MISSION]: åŸºäºå—é™ç¡¬ä»¶ï¼ˆRTX 5060 8GBï¼‰å®ç°è‡ªæ¼”åŒ–ã€é«˜å¯é æ€§çš„å­¦æœ¯åŠ©æ‰‹æã€‚
# [ARCHITECTURE]: HERO-A+ (Hierarchical Evolution & Reliability-Oriented)
# =================================================================

import yaml, time, threading, json, re, os, gc, traceback, random
from core.daily_reporter import DailyReporter

class Hanasuki:
    """
    Hanasuki æ ¸å¿ƒæ§åˆ¶ç±»ã€‚
    è´Ÿè´£è°ƒåº¦æ„Ÿæ€§è®°å¿† (Vector)ã€ç†æ€§é€»è¾‘ (Graph) ä¸ç‰©ç†æ‰§è¡Œå™¨ (Modules) æã€‚
    """
    def __init__(self):
        print("ğŸŒ¸ [å†…æ ¸] Hanasuki HERO-A+ æ¶æ„æ­£åœ¨åˆå§‹åŒ–...")
        from core.module_manager import ModuleManager
        from core.model_wrapper import get_model_backend
        from core.storage.vector_storage import VectorStorage
        from core.storage.graph_storage import GraphStorage

        # 1. åŸºç¡€èµ„æºè£…è½½æ (å¯¹é½ GUI å‘½åçš„ ModuleManager)
        self.config = self._load_config()
        self.model = get_model_backend(self.config['model'])
        
        # 2. æŒ‚è½½åŒè½¨è®°å¿†ä¸­æ¢ï¼šå‘é‡æ•°æ®åº“ (æ„Ÿæ€§) ä¸ é€»è¾‘å›¾è°± (ç†æ€§)
        self.vector_db = VectorStorage(self.config['modules'])
        self.graph_db = GraphStorage(self.config['modules'])
        
        # 3. æŒ‚è½½å·¥å…·ç®¡ç†ä¸æ—¥æŠ¥ç³»ç»Ÿæ
        self.mm = ModuleManager(self.config['modules']) 
        self.reporter = DailyReporter()

        # 4. åˆå§‹åŒ–ä¸Šä¸‹æ–‡çŠ¶æ€æœºä¸å¹¶å‘å®‰å…¨é”
        self.history = []
        self.learning_active = False
        self._lock = threading.Lock()
        
        # 5. ç‰©ç†ç¯å¢ƒè‡ªæ£€ï¼Œç¡®ä¿ data/ ç»“æ„å®Œæ•´æ
        self._check_environment()
        print("âœ… [å†…æ ¸] Hanasuki Beta 1.1 é€»è¾‘å¯¹é½å®Œæ¯•ï¼Œéšæ—¶å¾…å‘½æï¼")

    def _load_config(self):
        """[LOGIC]: åŠ è½½ config.yaml åŠ¨åŠ›å‚æ•°æ"""
        path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "config.yaml")
        with open(path, 'r', encoding='utf-8') as f: return yaml.safe_load(f)

    def _check_environment(self):
        """[SAFETY]: ç‰©ç†ç›®å½•å“¨å…µï¼Œé˜²æ­¢è¿è¡ŒæœŸå‡ºç° FileNotFoundError æ"""
        for d in ["data/logs", "data/reports", "data/vector_db", "workspace"]:
            os.makedirs(d, exist_ok=True)

    # --- [ACLA]: è‡ªé€‚åº”ä¸Šä¸‹æ–‡é€»è¾‘é”šå®šç®—æ³• ---
    def _trim_history_adaptive(self):
        """
        [ACLA - Adaptive Contextual Logic Anchoring]:
        ä¸åŒäºä¼ ç»Ÿçš„ FIFO (å…ˆè¿›å…ˆå‡º)ï¼ŒACLA é€šè¿‡æ•°å­¦è¯„ä¼°ä¿ç•™æœ€é‡è¦çš„é€»è¾‘é”šç‚¹æã€‚
        è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š
        $$S_{anchor} = \alpha \cdot \text{Recency} + \beta \cdot \frac{\text{Weight}}{\text{Threshold}}$$
        å…¶ä¸­ $\alpha=0.4$ (æ—¶é—´æƒé‡), $\beta=0.6$ (é€»è¾‘é‡è¦æ€§)ã€‚
        """
        if len(self.history) <= 15: return
        with self._lock:
            scored = []
            total_len = len(self.history)
            for i, msg in enumerate(self.history):
                # 1. è®¡ç®—æ—¶é—´æ–°é²œåº¦ (Recency)
                recency = (i + 1) / total_len
                # 2. æå–æ¶ˆæ¯ä¸­çš„å®ä½“å¹¶ä»å›¾è°±è·å–é€»è¾‘æƒé‡ (Logic Weight)
                entities = re.findall(r'[\u4e00-\u9fa5]{2,6}|[A-Za-z0-9\-]{3,15}', msg['content'])
                g_weight = sum([self.graph_db.get_node_importance(e) for e in entities])
                
                # 3. ç»¼åˆè¯„åˆ†ï¼Œæƒé‡é«˜çš„å®ä½“ (å¦‚ç§‘ç ”è¯¾é¢˜å…³é”®å®šä¹‰) å°†è¢«ç‰©ç†é”å®šåœ¨æ˜¾å­˜ä¸­æ
                score = 0.4 * recency + 0.6 * min(1.0, g_weight / 20.0)
                scored.append((score, msg))
            
            # æ’åºå¹¶ä¿ç•™å¾—åˆ†æœ€é«˜çš„ 12 æ¡æ ¸å¿ƒé”šç‚¹è®°å¿†ï¼Œä»¥åŠæœ€å 3 æ¡å®æ—¶å¯¹è¯æ
            scored.sort(key=lambda x: x[0], reverse=True)
            self.history = sorted([x[1] for x in scored[:12]] + self.history[-3:], 
                                 key=lambda x: self.history.index(x) if x in self.history else 999)

    # --- [TOOL]: Relign åè®®ä¸‹çš„ JSON è‡ªåŠ¨åŒ–æ‰§è¡Œé“¾æ¡ ---
    def _parse_and_execute(self, text):
        """
        [RELIGN Protocol]: ç‰©ç†æ‰§è¡Œ LLM åå‡ºçš„ JSON æŒ‡ä»¤æã€‚
        å¦‚æœè§£æåˆ° 'clarify'ï¼Œåˆ™è§¦å‘çŠ¹è±«æœºåˆ¶ï¼Œæ‹¦æˆªæ½œåœ¨çš„å·¥å…·å¹»è§‰ã€‚
        """
        pattern = r'```json\s*(\{.*?\})\s*```'
        match = re.search(pattern, text, re.DOTALL)
        if not match: return None
        
        try:
            call_data = json.loads(match.group(1))
            tool_name = call_data.get("tool")
            params = call_data.get("params", {})
            
            # [RELIGN]: æ‹¦æˆªä¸ç¡®å®šæ€§æ
            if tool_name == "clarify":
                return f"ã€Relign çŠ¹è±«è§¦å‘ã€‘: {params.get('reason')}"
            
            # ç‰©ç†è°ƒç”¨ ModuleManager è¿›è¡Œå…·èº«æ‰§è¡Œæ
            return self.mm.execute(tool_name, params)
        except:
            return "ï¼ˆHanasuki è¯•å›¾ç†è§£ JSONï¼Œä½†æ ¼å¼è§£æå¼‚å¸¸æ...ï¼‰"

    # --- [CORE]: å…·èº«åŒ–å¯¹è¯å·¥ä½œæµ ---
    def chat(self, user_input, stream=True):
        """
        [HYBRID RAG]: ç»“åˆå‘é‡æ£€ç´¢ (æ„Ÿæ€§) ä¸å›¾è°±é“¾æ¡ (ç†æ€§) çš„ç”Ÿæˆé€»è¾‘æã€‚
        """
        # 1. å¯åŠ¨ ACLA ä¸Šä¸‹æ–‡è‡ªé€‚åº”å‰ªè£ï¼Œä¿éšœ 8GB æ˜¾å­˜ç¨³å®šæ
        self._trim_history_adaptive()
        
        # 2. æ··åˆè¯­ä¹‰å¬å›
        v_ctx = self.vector_db.search_memory(user_input, limit=2) # æ£€ç´¢ç›¸è¿‘çš„èŠå¤©ç‰‡æ®µ
        entities = re.findall(r'[\u4e00-\u9fa5]{2,6}|[A-Za-z0-9\-]{3,15}', user_input)
        g_ctx = []
        for e in entities[:3]: g_ctx.extend(self.graph_db.query_logic_chain(e)) # æ£€ç´¢é€»è¾‘æ¨ç†é“¾æ¡
        
        # 3. æ„é€  HERO-A+ å¢å¼ºå‹ System Prompt
        sys_p = self._build_hero_prompt(v_ctx, g_ctx)
        msgs = [{"role": "system", "content": sys_p}] + self.history + [{"role": "user", "content": user_input}]
        
        # 4. ç”Ÿæˆå›å¤æµæ
        full_res = ""
        for chunk in self.model.generate(msgs, stream=stream):
            full_res += chunk
            yield chunk

        # 5. [NEW]: å®æ—¶å·¥å…·æ‰§è¡Œåé¦ˆ
        tool_result = self._parse_and_execute(full_res)
        if tool_result:
            yield f"\n\n> âš™ï¸ **å…·èº«æ‰§è¡Œåé¦ˆ**: {tool_result}"
            full_res += f"\n[Tool Output]: {tool_result}"

        # 6. å¯¹è¯å†…åŒ–ä¸æŒä¹…åŒ–å­˜å‚¨
        with self._lock:
            self.history.append({"role": "user", "content": user_input})
            self.history.append({"role": "assistant", "content": full_res})
        
        # å¼‚æ­¥æ›´æ–°è®°å¿†åº“ï¼Œä¸å¹²æ‰°ä¸»çº¿ç¨‹å“åº”æ
        threading.Thread(target=self._internalize, args=(user_input, full_res)).start()

    def _build_hero_prompt(self, v, g):
        """[LOGIC]: æ³¨å…¥å½“å‰å¬å›çš„çŸ¥è¯†ä¸ Relign ç¦ä»¤æ"""
        tools = self.mm.get_module_descriptions()
        return f"""ä½ ç°åœ¨æ˜¯ç®¡å®¶ Hanasukiã€‚
## å½“å‰å…³è”èƒŒæ™¯ï¼š
- ç›¸ä¼¼è®°å¿†ï¼š{v}
- é€»è¾‘å…³è”ï¼š{g}
## å¯ç”¨å·¥å…·æŒ‡ä»¤ï¼š
{tools}
## [Relign Protocol]ï¼š
å¦‚æœå·¥å…·å‚æ•°ï¼ˆURLã€æ–‡ä»¶åç­‰ï¼‰ä¸ç¡®å®šï¼Œä¸¥ç¦ç¼–é€ ï¼å¿…é¡»ä½¿ç”¨ clarify å·¥å…·è¯´æ˜å›°æƒ‘æã€‚"""

    def _internalize(self, u, a):
        """[LOGIC]: çŸ¥è¯†æ²‰æ·€ï¼Œæ›´æ–°å‘é‡åº“ä¸æ´»åŠ¨æ—¥å¿—æ"""
        self.vector_db.add_memory(f"Q: {u} | A: {a}")
        self.reporter.log_activity("Chat", f"ä¸å¤§å¤§åŒæ­¥äº†æœ€æ–°çš„é€»è¾‘èŠ‚ç‚¹æ")

    # --- [LEARNING]: è‡ªç ”æ¨¡å¼é€»è¾‘ ---
    def start_self_learning(self):
        """[LOGIC]: å¼€å¯ Hanasuki çš„â€œæ·±åº¦æ¢¦å¢ƒâ€è‡ªè¿›åŒ–æ¨¡å¼æ"""
        if self.learning_active: return
        self.learning_active = True
        threading.Thread(target=self._run_learning_loop, daemon=True).start()

    def _run_learning_loop(self):
        """[SELF-EVOLUTION]: è‡ªåŠ¨æ¢ç´¢é€»è¾‘å›¾è°±å¹¶è¡¥å…¨çŸ¥è¯†ç›²åŒºæ"""
        print("[è‡ªç ”] ğŸŒ™ æ­£åœ¨åˆ‡æ¢è‡³ 16k æ·±åº¦ä¸Šä¸‹æ–‡æ¨¡å¼...")
        self.model.reload(self.config['model']['profile_learning'])
        try:
            while self.learning_active:
                # 1. å¯»æ‰¾å›¾è°±ä¸­çš„è¾¹ç¼˜èŠ‚ç‚¹æ
                target = self.graph_db.get_random_node()
                if not target: time.sleep(10); continue
                
                # 2. æ¨¡æ‹Ÿç ”ç©¶è¿‡ç¨‹
                res = self.model.generate([{"role": "user", "content": f"æ·±å…¥åˆ†æå¹¶æ¼”åŒ–å®ä½“é€»è¾‘: {target}"}], stream=False)
                tool_res = self._parse_and_execute(res)
                
                # 3. å¤„ç†ç ”ç©¶éšœç¢æ (å¦‚æœå¡ä½åˆ™è®°å…¥æ—©æŠ¥)
                if "ã€Relign çŠ¹è±«è§¦å‘ã€‘" in str(tool_res):
                    self.reporter.log_confusion(target, tool_res)
                else:
                    self.graph_db.add_relation(target, "å·²æ¼”åŒ–", "Academic_Node")
                
                # 4. æ˜¾å­˜çƒ­é‡å¹³è¡¡ä¼‘çœ 
                time.sleep(self.config.get('learning', {}).get('idle_threshold', 30))
        finally:
            self.model.reload(self.config['model']['profile_normal'])